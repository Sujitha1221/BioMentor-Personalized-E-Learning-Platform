import requests
import logging
import faiss
import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from utils.quiz_generation_methods import assign_difficulty_parameter, assign_discrimination_parameter, is_question_too_similar, retrieve_context_questions
from routes.response_routes import estimate_student_ability

# Load Sentence Transformer model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Load FAISS index and dataset
index = faiss.read_index("dataset/question_embeddings.index")
embeddings_matrix = np.load("dataset/question_embeddings.npy")
dataset = pd.read_csv("dataset/question_dataset_with_clusters.csv")

# Google Colab API Endpoint
COLAB_API_URL = "https://fc8d-34-125-70-103.ngrok-free.app/generate_mcq"

# Method to generate MCQs with unique context
def generate_mcq(difficulty, user_id, max_retries=3):  # ðŸ”¥ Reduced retries to 3
    """Generates an MCQ while ensuring uniqueness via FAISS and diverse context retrieval."""
    retries = 0
    logging.info(f"ðŸŽ¯ Attempting to generate an MCQ for difficulty: {difficulty}")

    while retries < max_retries:
        try:
            # âœ… Check if dataset is empty before sampling
            if dataset.empty:
                logging.error("ðŸ”¥ ERROR: Dataset is empty. Cannot generate MCQ.")
                return None
            
            sampled_question = dataset.sample(1)

            if sampled_question.empty:
                logging.error("ðŸ”¥ ERROR: No questions available in dataset. Retrying...")
                retries += 1
                continue  

            random_question = sampled_question.iloc[0]["Question Text"]
            context_questions = retrieve_context_questions(random_question, top_k=3)

            # âœ… Construct context-based prompt
            context_list = [
                f"- {row['Question Text']} (Correct Answer: {row['Correct Answer']})"
                for _, row in context_questions.iterrows()
            ] if not context_questions.empty else []

            prompt = f"""
            Generate a **{difficulty}** level multiple-choice biology question.

            **Requirements:**
            - Must be a completely new and unique question.
            - Must contain exactly **5 unique answer choices** labeled **A-E**.
            - Must explicitly state the correct answer.
            - The correct answer must be **one of the provided choices**.

            **Format:**
            ```
            Question: <Generated Question>
            A) <Option A>
            B) <Option B>
            C) <Option C>
            D) <Option D>
            E) <Option E>
            Correct Answer: <A/B/C/D/E>
            ```
            """

            # âœ… Add context if available
            if context_list:
                prompt = f"""
                Generate a **{difficulty}** level MCQ that is **different** from these:

                {''.join(context_list)}

                **Follow This Format:**
                {prompt}
                """

            # âœ… Send API request (Improved Error Handling)
            try:
                response = requests.post(COLAB_API_URL, json={"prompt": prompt}, timeout=30)
                response.raise_for_status()  # ðŸ”¥ Handle HTTP errors
                data = response.json()
            except (requests.exceptions.RequestException, ValueError) as e:
                logging.error(f"âš  API Error: {e}")
                retries += 1
                continue

            # âœ… Debug: Log API response
            logging.warning(f"âš  RAW API RESPONSE: {data}")

            # âœ… Ensure response is correctly formatted
            if not isinstance(data, dict) or "mcq" not in data:
                logging.error(f"ðŸ”¥ ERROR: Invalid API response format. Retrying...")
                retries += 1
                continue

            question_data = data["mcq"]
            
            # âœ… Handle API errors in response
            if "error" in question_data:
                logging.warning(f"âš  MCQ API Error: {question_data['error']}. Retrying...")
                retries += 1
                continue

            # âœ… Ensure the question exists and isn't a placeholder
            question_text = question_data.get("question", "").strip()
            if not question_text or "<Generated Question>" in question_text:
                logging.warning(f"âš  Malformed MCQ detected: {question_data}. Retrying...")
                retries += 1
                continue

            # âœ… Ensure answer choices are unique
            options = question_data.get("options", {})
            
            if any("<Option" in opt for opt in options.values()):
                logging.warning(f"âš  Placeholder detected in options: {options}. Retrying...")
                retries += 1
                continue 
            
            if len(set(options.values())) < 5:  # ðŸ”¥ Check for duplicate answers
                logging.warning(f"âš  Duplicate answer choices detected: {options}")
                retries += 1
                continue  

            # âœ… Ensure correct answer exists and is valid
            correct_answer = question_data.get("correct_answer", "").strip()
            if correct_answer not in ["A", "B", "C", "D", "E"] or options.get(correct_answer, "") not in options.values():
                logging.warning(f"âš  Incorrect correct answer: {correct_answer} not in {options}")
                retries += 1
                continue  

            # âœ… Ensure question is unique (Avoid FAISS similarity failures)
            if is_question_too_similar(question_text):
                logging.warning(f"âš  Too Similar: {question_text}")
                retries += 1
                continue  

            # âœ… Add difficulty level
            question_data["difficulty"] = difficulty

            # âœ… Assign difficulty parameters
            question_data["b"] = assign_difficulty_parameter(user_id, difficulty)
            question_data["a"] = assign_discrimination_parameter()
            question_data["c"] = 0.2  

            # âœ… Store in FAISS
            new_vector = embedding_model.encode([question_text]).astype(np.float32)
            index.add(new_vector)

            logging.info(f"âœ… Successfully generated MCQ: {question_text}")
            return question_data

        except Exception as e:
            logging.error(f"âš  Unexpected Error: {e}")
            retries += 1

    logging.error("âŒ Failed to generate a unique MCQ after multiple attempts.")
    return None  # ðŸ”¥ Return None instead of an error


def generate_mcq_based_on_performance(user_id, difficulty, max_retries=5):
    """Generates a new MCQ dynamically based on user's past performance and assigns IRT parameters using ability level."""
    retries = 0
    logging.info(f"ðŸŽ¯ Attempting to generate an MCQ for difficulty: {difficulty}")

    while retries < max_retries:
        # âœ… Fetch user ability
        theta = estimate_student_ability(user_id) or 0.0  # Default if None
        try:
            # âœ… Check if dataset is empty before sampling
            if dataset.empty:
                logging.error("ðŸ”¥ ERROR: Dataset is empty. Cannot generate MCQ.")
                return None
            
            sampled_question = dataset.sample(1)

            if sampled_question.empty:
                logging.error("ðŸ”¥ ERROR: No questions available in dataset. Retrying...")
                retries += 1
                continue  

            random_question = sampled_question.iloc[0]["Question Text"]
            context_questions = retrieve_context_questions(random_question, top_k=3)

            # âœ… Construct context-based prompt
            context_list = [
                f"- {row['Question Text']} (Correct Answer: {row['Correct Answer']})"
                for _, row in context_questions.iterrows()
            ] if not context_questions.empty else [] 
           
            prompt = f"""
            Generate a **{difficulty}** level multiple-choice biology question.
        
            **User's Estimated Ability Level (IRT Theta):** {theta}
        
            **Requirements:**
            - Must be a completely new and unique question.
            - Must contain exactly **5 unique answer choices** labeled **A-E**.
            - Must explicitly state the correct answer.
            - The correct answer must be **one of the provided choices**.

            **Format:**
        
            Question: <Generated Question>
            A) <Option A>
            B) <Option B>
            C) <Option C>
            D) <Option D>
            E) <Option E>
            Correct Answer: <A/B/C/D/E>

            """
        
            # âœ… Add context if available
            if context_list:
                prompt = f"""
                Generate a **{difficulty}** level MCQ that is **different** from these:

                {''.join(context_list)}

                **Follow This Format:**
                {prompt}
                """

            try:
                response = requests.post(COLAB_API_URL, json={"prompt": prompt}, timeout=30)
                response.raise_for_status()  # ðŸ”¥ Handle HTTP errors
                data = response.json()
            except (requests.exceptions.RequestException, ValueError) as e:
                logging.error(f"âš  API Error: {e}")
                retries += 1
                continue

            # âœ… Debug: Log API response
            logging.warning(f"âš  RAW API RESPONSE: {data}")

            # âœ… Ensure response is correctly formatted
            if not isinstance(data, dict) or "mcq" not in data:
                logging.error(f"ðŸ”¥ ERROR: Invalid API response format. Retrying...")
                retries += 1
                continue

            question_data = data["mcq"]

            # âœ… Handle API errors in response
            if "error" in question_data:
                logging.warning(f"âš  MCQ API Error: {question_data['error']}. Retrying...")
                retries += 1
                continue

            # âœ… Ensure the question exists and isn't a placeholder
            question_text = question_data.get("question", "").strip()
            if not question_text or "<Generated Question>" in question_text:
                logging.warning(f"âš  Malformed MCQ detected: {question_data}. Retrying...")
                retries += 1
                continue

            # âœ… Ensure answer choices are unique
            options = question_data.get("options", {})
            
            if any("<Option" in opt for opt in options.values()):
                logging.warning(f"âš  Placeholder detected in options: {options}. Retrying...")
                retries += 1
                continue 
            
            if len(set(options.values())) < 5:  # ðŸ”¥ Check for duplicate answers
                logging.warning(f"âš  Duplicate answer choices detected: {options}")
                retries += 1
                continue  

            # âœ… Ensure correct answer is valid
            correct_answer = question_data.get("correct_answer", "").strip()
            answer_choices = ["A", "B", "C", "D", "E"]
            if correct_answer not in answer_choices or options.get(correct_answer, "") not in options.values():
                logging.warning(f"âš  Correct answer mismatch: {correct_answer} not in {options}")
                retries += 1
                continue

            # âœ… Ensure question is unique (Avoid FAISS similarity failures)
            if is_question_too_similar(question_text):
                logging.warning(f"âš  Too Similar: {question_text}")
                retries += 1
                continue  

            # âœ… Add difficulty to each question
            question_data["difficulty"] = difficulty

            # âœ… Assign IRT parameters dynamically
            question_data["b"] = assign_difficulty_parameter(user_id, difficulty)  # Pass correct user_id
            question_data["a"] = assign_discrimination_parameter()
            question_data["c"] = 0.2  # Fixed guessing parameter

            # âœ… Store in FAISS
            new_vector = embedding_model.encode([question_text]).astype(np.float32)
            index.add(new_vector)
        
            logging.info(f"âœ… Successfully generated MCQ: {question_data['question']}")
            return question_data

    # âœ… Handle unexpected errors
        except Exception as e:
            logging.error(f"âš  Unexpected Error: {e}")
            retries += 1  # âœ… Make sure this line is indented properly

    logging.error("âŒ Failed to generate a unique MCQ after multiple attempts.")
    return None  # ðŸ”¥ Return None instead of an error

