{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9UjjGdeNQeBV",
    "outputId": "19b707c2-7f17-495b-d271-310ee3335b92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\dharane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.9.0.post1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: language-tool-python in c:\\users\\dharane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.8.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\dharane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\dharane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: pip in c:\\users\\dharane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-tool-python) (23.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\dharane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-tool-python) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dharane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-tool-python) (4.66.4)\n",
      "Requirement already satisfied: wheel in c:\\users\\dharane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from language-tool-python) (0.45.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dharane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->language-tool-python) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dharane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->language-tool-python) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dharane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->language-tool-python) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dharane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->language-tool-python) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\dharane\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->language-tool-python) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu language-tool-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSGAmKe2RZuv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data():\n",
    "    # Load dataset\n",
    "    dataset1 = '../Model-Training/bio_summary_keywords.csv'\n",
    "    dataset2 = '../Model-Training/biology_information_retrieval_sample.csv'\n",
    "\n",
    "    # Load summarization dataset\n",
    "    summary_df = pd.read_csv(dataset1, encoding='ISO-8859-1')\n",
    "    long_texts = summary_df['Long Text'].tolist()\n",
    "    summaries = summary_df['Summary'].tolist()\n",
    "    keywords = summary_df['Keywords'].tolist()\n",
    "\n",
    "    # Load notes dataset\n",
    "    notes_df = pd.read_csv(dataset2, encoding='ISO-8859-1')\n",
    "    notes_content = notes_df['Text Content'].tolist()\n",
    "    notes_topics = notes_df['Topic'].tolist()\n",
    "    notes_subtopics = notes_df['Sub-topic'].tolist()\n",
    "\n",
    "    return long_texts, summaries, keywords, notes_content\n",
    "\n",
    "# Load the data\n",
    "long_texts, summaries, keywords, notes_content = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mX6Q-p2Rjuv5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Path to the fine-tuned model\n",
    "model_path = 'D:/Downloads/RP/Summarization/flan_t5_finetuned_model-20241119T102614Z-001/flan_t5_finetuned_model'\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XbQPQzRVRdhw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "def initialize_faiss(long_texts, notes_content, embedder):\n",
    "    # Generate embeddings for long texts and notes\n",
    "    long_text_embeddings = embedder.encode(long_texts)\n",
    "    notes_embeddings = embedder.encode(notes_content)\n",
    "\n",
    "    # Combine embeddings\n",
    "    all_embeddings = np.concatenate([long_text_embeddings, notes_embeddings], axis=0)\n",
    "\n",
    "    # Create FAISS index\n",
    "    index = faiss.IndexFlatL2(all_embeddings.shape[1])\n",
    "    index.add(all_embeddings)\n",
    "\n",
    "    return index\n",
    "\n",
    "# Initialize Sentence Embedder and FAISS Index\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "faiss_index = initialize_faiss(long_texts, notes_content, embedder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "F2fcw2TvlXjg"
   },
   "outputs": [],
   "source": [
    "def postprocess_summary(summary):\n",
    "    \"\"\"Capitalize the first letter of each sentence.\"\"\"\n",
    "    summary = summary.strip()\n",
    "\n",
    "    # Capitalize first letter of each sentence\n",
    "    sentences = summary.split(\". \") \n",
    "    sentences = [s.strip().capitalize() for s in sentences if s]\n",
    "\n",
    "    # Rejoin sentences with proper spacing and punctuation\n",
    "    summary = \". \".join(sentences).strip()\n",
    "\n",
    "    # Ensure final punctuation\n",
    "    if summary and summary[-1] not in \".!?\":\n",
    "        summary += \".\"\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "3c3jKSSjRg_Q"
   },
   "outputs": [],
   "source": [
    "def retrieve_relevant_content(query, embedder, index, long_texts, summaries):\n",
    "    # Query embedding\n",
    "    query_embedding = embedder.encode([query]).astype(\"float32\")\n",
    "\n",
    "    # Search the FAISS index\n",
    "    distances, indices = index.search(query_embedding, k=3)\n",
    "\n",
    "    # Retrieve the top results\n",
    "    relevant_texts = []\n",
    "    for idx in indices[0]:\n",
    "        if idx < len(long_texts):  # Found in the long texts\n",
    "            relevant_texts.append(long_texts[idx])\n",
    "\n",
    "    return relevant_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "k4rqBHE3XBiO"
   },
   "outputs": [],
   "source": [
    "# Function to ensure the summary fits within the desired word count range and finishes with a complete sentence.\"\"\"\n",
    "\n",
    "def truncate_to_word_count(text, max_words):\n",
    "    words = text.split()\n",
    "\n",
    "    if len(words) > max_words:\n",
    "        truncated_text = \" \".join(words[:max_words])\n",
    "\n",
    "        # Find the last punctuation efficiently\n",
    "        for i in range(len(truncated_text) - 1, -1, -1):\n",
    "            if truncated_text[i] in \".!?\":\n",
    "                return truncated_text[:i + 1]  # Return up to the last punctuation\n",
    "\n",
    "        # If no punctuation is found, return strict truncation\n",
    "        return \" \".join(words[:max_words - 1]).strip()\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "YEYgnptRSM9b"
   },
   "outputs": [],
   "source": [
    "def generate_summary_for_long_text(long_text, min_words=100, max_words=250):\n",
    "    from textwrap import wrap\n",
    "\n",
    "    # Helper function to chunk text\n",
    "    def chunk_text(text, max_tokens=500):\n",
    "        words = text.split()\n",
    "        chunks = [' '.join(words[i:i + max_tokens]) for i in range(0, len(words), max_tokens)]\n",
    "        return chunks\n",
    "\n",
    "    # Check if input exceeds the max token limit\n",
    "    max_input_words = 390  # ~512 tokens\n",
    "    if len(long_text.split()) > max_input_words:\n",
    "        # Chunk the input into smaller parts\n",
    "        chunks = chunk_text(long_text, max_tokens=max_input_words)\n",
    "\n",
    "        # Generate a summary for each chunk and combine the results\n",
    "        summaries = [generate_summary_for_long_text(chunk, min_words, max_words) for chunk in chunks]\n",
    "        combined_summary = \" \".join(summaries)\n",
    "\n",
    "        # Ensure the combined summary fits within the final word range\n",
    "        return truncate_to_word_count(combined_summary, max_words)\n",
    "\n",
    "    # For shorter inputs, generate the summary directly\n",
    "    prompt = (\n",
    "        f\"Generate a concise, well-structured, and grammatically correct summary for the following content:\\n\\n\"\n",
    "        f\"{long_text}\\n\\nSummary:\"\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_words * 2,   # Allow for token-to-word conversion (~1.3x)\n",
    "        min_length=min_words,      # Enforce minimum token count\n",
    "        length_penalty=1.2,\n",
    "        num_beams=4,\n",
    "        repetition_penalty=2.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    summary = postprocess_summary(summary)\n",
    "\n",
    "    # Truncate summary to fit exact word count range and finish the last sentence\n",
    "    return truncate_to_word_count(summary, max_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "nyhDXsw1SiWi"
   },
   "outputs": [],
   "source": [
    "# Function to generate a summary\n",
    "def generate_summary(query, min_words=100, max_words=250):\n",
    "\n",
    "    prompt = f\"Summarize the following content related to '{query}':\\n{query}\\nSummary:\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_words,\n",
    "        min_length=min_words,\n",
    "        length_penalty=1.5,\n",
    "        num_beams=4,\n",
    "        repetition_penalty=3.0,\n",
    "        early_stopping=True,\n",
    "    )\n",
    "\n",
    "    # Decode the summary\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "lBHUe6XgSoHt"
   },
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    # Load data\n",
    "    long_texts, summaries, keywords, notes_content = load_data()\n",
    "\n",
    "    # Load the embedding model and initialize FAISS index\n",
    "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    faiss_index = initialize_faiss(long_texts, notes_content, embedder)\n",
    "\n",
    "# Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBSaUNOUuy9S",
    "outputId": "2b69fa5f-6d28-4a52-8ac6-07fd6206ebf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Summary:\n",
      "The heart is a roughly cone-shaped hollow muscular organ. It is about 10 cm long and weighs about 225 g in women and 310 g in men. It lies in the thoracic cavity in the mediastinum between the lungs anteriorly the sternum, ribs and intercostal muscles structure the heart is composed of three layers pericardium, myocardium and endocardium. The outer sac consists of fibrous tissue and the inner of a continuous double layer of serous membrane. Cardiac muscle is found only in the heart. It is not under voluntary control but crossstripes are seen on microscopic examination. Each fibre cell has a nucleus and one or more branches. The ends of the cells and their branches are in close contact with the ends and branches of adjacent cells. When an impulse is initiated it spreads from cell to cell via the branches and intercalated discs over the whole sheet of muscle causing contraction. The myocardium is thickest at the apex and thins out towards the base. The atria and ventricles are separated by a ring of fibrous tissue that does not conduct electrical impulses. The endocardium is a thin, smooth membrane which allows smooth flow of blood inside the heart. Heart failure occurs when the cardiac output is unable to maintain sufficient blood supply to meet the needs of the body.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve relevant content\n",
    "query = \"Heart\"\n",
    "relevant_texts = retrieve_relevant_content(query, embedder, faiss_index, long_texts, summaries)\n",
    "\n",
    "# Generate summary based on relevance\n",
    "if relevant_texts:\n",
    "    # Join all relevant texts into one string before passing to summary function\n",
    "    long_text_combined = \" \".join(relevant_texts)\n",
    "\n",
    "    summary = generate_summary_for_long_text(long_text_combined, min_words=100, max_words=250)\n",
    "else:\n",
    "    summary = \"No relevant content found.\"\n",
    "\n",
    "# Print the final cleaned summary\n",
    "print(\"Cleaned Summary:\")\n",
    "print(summary)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
